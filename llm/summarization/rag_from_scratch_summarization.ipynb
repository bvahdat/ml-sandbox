{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Lance Martin](https://github.com/rlancemartin) from [LangChain](https://www.langchain.com/) has recently uploaded awesome [YouTube videos about RAG](https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x) with [accompanying codebase](https://github.com/langchain-ai/rag-from-scratch).<br><br>By the following codebase we generate a summarization of it's transcripts using the MapReduce approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import OpenAIWhisperParser\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pytube import Playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "# os.environ['LANGCHAIN_API_KEY'] = <your-api-key>\n",
    "# os.environ['OPENAI_API_KEY'] = <your-api-key>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the videos first through the given playlist on YouTube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retrieved 14 single videos from the playlist'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlist_url = 'https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x'\n",
    "playlist = Playlist(playlist_url)\n",
    "urls = [url for url in playlist]\n",
    "f'retrieved {len(urls)} single videos from the playlist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the YouTube URLs as audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=wd7TZ4w1mSw\n",
      "[youtube] wd7TZ4w1mSw: Downloading webpage\n",
      "[youtube] wd7TZ4w1mSw: Downloading ios player API JSON\n",
      "[youtube] wd7TZ4w1mSw: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"wd7TZ4w1mSw\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] wd7TZ4w1mSw: Downloading m3u8 information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Incomplete data received in embedded initial data; re-fetching using API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] wd7TZ4w1mSw: Downloading initial data API JSON\n",
      "[info] wd7TZ4w1mSw: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 1 (Overview).m4a has already been downloaded\n",
      "[download] 100% of    4.82MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 1 (Overview).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=bjb_EMsTDKI\n",
      "[youtube] bjb_EMsTDKI: Downloading webpage\n",
      "[youtube] bjb_EMsTDKI: Downloading ios player API JSON\n",
      "[youtube] bjb_EMsTDKI: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"bjb_EMsTDKI\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] bjb_EMsTDKI: Downloading m3u8 information\n",
      "[info] bjb_EMsTDKI: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 2 (Indexing).m4a has already been downloaded\n",
      "[download] 100% of    4.50MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 2 (Indexing).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=LxNVgdIz9sU\n",
      "[youtube] LxNVgdIz9sU: Downloading webpage\n",
      "[youtube] LxNVgdIz9sU: Downloading ios player API JSON\n",
      "[youtube] LxNVgdIz9sU: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"LxNVgdIz9sU\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] LxNVgdIz9sU: Downloading m3u8 information\n",
      "[info] LxNVgdIz9sU: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 3 (Retrieval).m4a has already been downloaded\n",
      "[download] 100% of    4.84MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 3 (Retrieval).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Vw52xyyFsB8\n",
      "[youtube] Vw52xyyFsB8: Downloading webpage\n",
      "[youtube] Vw52xyyFsB8: Downloading ios player API JSON\n",
      "[youtube] Vw52xyyFsB8: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"Vw52xyyFsB8\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Vw52xyyFsB8: Downloading m3u8 information\n",
      "[info] Vw52xyyFsB8: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 4 (Generation).m4a has already been downloaded\n",
      "[download] 100% of    5.94MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 4 (Generation).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=JChPi0CRnDY\n",
      "[youtube] JChPi0CRnDY: Downloading webpage\n",
      "[youtube] JChPi0CRnDY: Downloading ios player API JSON\n",
      "[youtube] JChPi0CRnDY: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"JChPi0CRnDY\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] JChPi0CRnDY: Downloading m3u8 information\n",
      "[info] JChPi0CRnDY: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 5 (Query Translation -- Multi Query).m4a has already been downloaded\n",
      "[download] 100% of    5.69MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 5 (Query Translation -- Multi Query).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=77qELPbNgxA\n",
      "[youtube] 77qELPbNgxA: Downloading webpage\n",
      "[youtube] 77qELPbNgxA: Downloading ios player API JSON\n",
      "[youtube] 77qELPbNgxA: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"77qELPbNgxA\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] 77qELPbNgxA: Downloading m3u8 information\n",
      "[info] 77qELPbNgxA: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 6 (Query Translation -- RAG Fusion).m4a has already been downloaded\n",
      "[download] 100% of    5.27MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 6 (Query Translation -- RAG Fusion).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=h0OPWlEOank\n",
      "[youtube] h0OPWlEOank: Downloading webpage\n",
      "[youtube] h0OPWlEOank: Downloading ios player API JSON\n",
      "[youtube] h0OPWlEOank: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"h0OPWlEOank\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] h0OPWlEOank: Downloading m3u8 information\n",
      "[info] h0OPWlEOank: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 7 (Query Translation -- Decomposition).m4a has already been downloaded\n",
      "[download] 100% of    6.12MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 7 (Query Translation -- Decomposition).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=xn1jEjRyJ2U\n",
      "[youtube] xn1jEjRyJ2U: Downloading webpage\n",
      "[youtube] xn1jEjRyJ2U: Downloading ios player API JSON\n",
      "[youtube] xn1jEjRyJ2U: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"xn1jEjRyJ2U\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] xn1jEjRyJ2U: Downloading m3u8 information\n",
      "[info] xn1jEjRyJ2U: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 8 (Query Translation -- Step Back).m4a has already been downloaded\n",
      "[download] 100% of    6.45MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 8 (Query Translation -- Step Back).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=SaDzIVkYqyY\n",
      "[youtube] SaDzIVkYqyY: Downloading webpage\n",
      "[youtube] SaDzIVkYqyY: Downloading ios player API JSON\n",
      "[youtube] SaDzIVkYqyY: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"SaDzIVkYqyY\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] SaDzIVkYqyY: Downloading m3u8 information\n",
      "[info] SaDzIVkYqyY: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 9 (Query Translation -- HyDE).m4a has already been downloaded\n",
      "[download] 100% of    4.42MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 9 (Query Translation -- HyDE).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=pfpIndq7Fi8\n",
      "[youtube] pfpIndq7Fi8: Downloading webpage\n",
      "[youtube] pfpIndq7Fi8: Downloading ios player API JSON\n",
      "[youtube] pfpIndq7Fi8: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"pfpIndq7Fi8\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] pfpIndq7Fi8: Downloading m3u8 information\n",
      "[info] pfpIndq7Fi8: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 10 (Routing).m4a has already been downloaded\n",
      "[download] 100% of    6.52MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 10 (Routing).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=kl6NwWYxvbM\n",
      "[youtube] kl6NwWYxvbM: Downloading webpage\n",
      "[youtube] kl6NwWYxvbM: Downloading ios player API JSON\n",
      "[youtube] kl6NwWYxvbM: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"kl6NwWYxvbM\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] kl6NwWYxvbM: Downloading m3u8 information\n",
      "[info] kl6NwWYxvbM: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 11 (Query Structuring).m4a has already been downloaded\n",
      "[download] 100% of    5.53MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 11 (Query Structuring).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=gTCU9I6QqCE\n",
      "[youtube] gTCU9I6QqCE: Downloading webpage\n",
      "[youtube] gTCU9I6QqCE: Downloading ios player API JSON\n",
      "[youtube] gTCU9I6QqCE: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"gTCU9I6QqCE\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] gTCU9I6QqCE: Downloading m3u8 information\n",
      "[info] gTCU9I6QqCE: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 12 (Multi-Representation Indexing).m4a has already been downloaded\n",
      "[download] 100% of    6.09MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG from scratch： Part 12 (Multi-Representation Indexing).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=z_6EeA2LDSw\n",
      "[youtube] z_6EeA2LDSw: Downloading webpage\n",
      "[youtube] z_6EeA2LDSw: Downloading ios player API JSON\n",
      "[youtube] z_6EeA2LDSw: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"z_6EeA2LDSw\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] z_6EeA2LDSw: Downloading m3u8 information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Incomplete data received in embedded initial data; re-fetching using API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] z_6EeA2LDSw: Downloading initial data API JSON\n",
      "[info] z_6EeA2LDSw: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 13 (RAPTOR).m4a has already been downloaded\n",
      "[download] 100% of    7.09MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 13 (RAPTOR).m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=cN6S0Ehm7_8\n",
      "[youtube] cN6S0Ehm7_8: Downloading webpage\n",
      "[youtube] cN6S0Ehm7_8: Downloading ios player API JSON\n",
      "[youtube] cN6S0Ehm7_8: Downloading android player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"cN6S0Ehm7_8\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] cN6S0Ehm7_8: Downloading m3u8 information\n",
      "[info] cN6S0Ehm7_8: Downloading 1 format(s): 140\n",
      "[download] /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 14 (ColBERT).m4a has already been downloaded\n",
      "[download] 100% of    6.67MiB\n",
      "[ExtractAudio] Not converting audio /Users/bvahdat/Downloads/rag_from_scratch_audios/RAG From Scratch： Part 14 (ColBERT).m4a; file is already in target format m4a\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n",
      "Transcribing part 1!\n"
     ]
    }
   ],
   "source": [
    "loader = GenericLoader(YoutubeAudioLoader(urls, save_dir='~/Downloads/rag_from_scratch_audios'), OpenAIWhisperParser())\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the MapReduce approach for transcript summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt using the MapReduce approach\n",
    "map_prompt_template = '''\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      '''\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=['text'])\n",
    "\n",
    "combine_prompt_template = '''\n",
    "                      Write a concise summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      '''\n",
    "\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=['text'])\n",
    "\n",
    "# LLM (unfortunatley setting the batch_size of 1 is not possible to keep the videos order)\n",
    "# see https://github.com/langchain-ai/langchain/issues/2465\n",
    "llm = ChatOpenAI(model_name='gpt-4-turbo', temperature=0)\n",
    "\n",
    "# Chain\n",
    "map_reduce_chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "# Formatting\n",
    "def output_content(content, title_prefix):\n",
    "    for i, doc in enumerate(content):\n",
    "        print(f'{title_prefix} Nr. {i+1}:')\n",
    "        print(doc.page_content if isinstance(doc, Document) else doc)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/rag-from-scratch/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "map_reduce_outputs = map_reduce_chain({'input_documents': docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short summarization over all the videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Lance from LangChain explores query translation techniques in the \"RAG from Scratch\" series to enhance information retrieval in Retrieval Augmentation Graphs (RAGs).\n",
      "- Key strategies for query translation include:\n",
      "  1. Rewriting Questions: Modifying the original question to explore different perspectives.\n",
      "  2. Sub-Question Breakdown: Splitting complex questions into simpler ones, solving them separately, and combining the results.\n",
      "  3. Step-Back Prompting: Creating more abstract questions from specific ones to improve document retrieval.\n",
      "- The series covers advanced topics such as using metadata filters for structured queries, the Raptor technique for hierarchical indexing, and the HIDE technique for transforming questions into hypothetical documents.\n",
      "- Techniques like logical and semantic routing are discussed to direct modified questions to the right data sources.\n",
      "- Multi-representation indexing is introduced, where documents are distilled into propositions using a Large Language Model (LLM) to optimize retrieval.\n",
      "- The fourth video focuses on integrating retrieved documents into the LLM context window for answer generation, using techniques like K-Nearest Neighbors.\n",
      "- A prompt template is created to organize retrieved documents and facilitate answer generation, demonstrated using the LangChain Expression Language (LCEL).\n",
      "- Future videos will explore more complex themes and address limitations of the current simple pipeline.\n",
      "- The multi-query method is highlighted, where multiple versions of a question are generated, documents retrieved for each, and results combined to enhance retrieval reliability.\n",
      "- Rack Fusion is discussed, using reciprocal rank fusion to consolidate results from multiple retrievals into a single ranked list.\n",
      "- The Colbert indexing approach is introduced, involving tokenizing documents and questions and creating embeddings with positional weighting, demonstrated using the Rakutui library.\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_outputs['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization of each single video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization text of the video Nr. 1:\n",
      "This text is a transcript from Lance at LangChain, discussing the concept of step-back prompting in the context of query translation for retrieval augmentation graphs (RAGs). The main focus is on improving the retrieval process by modifying input questions to enhance the relevance and comprehensiveness of the retrieved information.\n",
      "\n",
      "The video explores different strategies for query translation:\n",
      "1. **Rewriting Questions**: This involves modifying the original question to capture various perspectives, potentially improving the retrieval process. Techniques like RAG fusion and multi-query are examples of this approach.\n",
      "2. **Sub-Question Breakdown**: This method involves decomposing a complex question into simpler sub-questions, solving each independently, and then consolidating the answers.\n",
      "3. **Step-Back Prompting**: Introduced by Google, this technique involves formulating more abstract questions from the specific ones. It uses few-shot prompting to generate \"step-back\" or more abstract questions, which are broader and potentially easier to answer, thus improving document retrieval.\n",
      "\n",
      "The process of step-back prompting is detailed with examples and a practical walkthrough. Lance explains how to use a template to generate a more generic question from a specific one, perform retrieval for both the original and the step-back question, and then combine the results to produce a final answer. This approach is particularly useful in domains where conceptual knowledge is significant, such as technical documentation or educational materials.\n",
      "\n",
      "Overall, the video serves as a tutorial on enhancing query translation through step-back prompting, aiming to improve the efficiency and accuracy of information retrieval in RAG systems.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 2:\n",
      "This text is a transcript from the 11th part of the \"RAG from Scratch\" video series by Lance from LangChain, focusing on query construction. The series previously covered topics like query translation and routing, which involve optimizing and directing questions to appropriate data sources such as vector stores, GraphDB, or SQLDB. In this episode, Lance discusses converting natural language into domain-specific queries for vector stores, specifically using metadata filters.\n",
      "\n",
      "The main problem addressed is how to structure a natural language query into a format that can be used by vector stores to retrieve relevant data. For example, converting a question like \"find me videos on chat LangChain published after 2024\" into a structured query that utilizes metadata filters in a vector store to fetch data based on specified criteria such as topic and publication date.\n",
      "\n",
      "To achieve this, function calling with models like OpenAI is used. The process involves taking metadata fields from the vector store and providing them to the model, which then generates queries adhering to the schema provided. These queries are then parsed into structured objects, such as pedantic objects, which can be used in searches.\n",
      "\n",
      "Lance demonstrates this process using a notebook, showing how metadata from a YouTube video can be used to set up filters and searches on various attributes like view count, publication date, and video length. He also explains how to encapsulate this information in a tutorial search object, which helps in building database queries optimized for retrieval.\n",
      "\n",
      "The video concludes with examples of how this method can be applied to generate structured queries from natural language inputs, emphasizing its broad applicability and convenience for querying different vector databases. Lance encourages viewers to experiment with this technique and provides links to documentation for further learning.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 3:\n",
      "Lance from LangChain introduces the Raptor technique in the 13th part of the RAG from Scratch series, focusing on hierarchical indexing within vector stores. Raptor is designed to address the challenge of retrieving information from multiple document chunks simultaneously, which may exceed the typical k-nearest-neighbors retrieval parameter. The technique involves creating a hierarchical index of document summaries by clustering similar documents and summarizing each cluster. This process is done recursively until a single cluster or a predefined limit is reached, resulting in a high-level summary of all documents.\n",
      "\n",
      "The indexed summaries span various levels of detail, from individual document chunks to broader summaries, enhancing semantic search capabilities by matching the abstraction level of the query with the appropriate chunk or summary. Lance demonstrates the application of the Raptor technique on a set of LangChain documents, detailing the steps of embedding, clustering, and recursive summarization. He also mentions using tools like Claude and OpenAI for implementation, and provides additional resources such as a deep dive video and a full code notebook for those interested in a more detailed exploration of the technique.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 4:\n",
      "Lance from Langchain introduces a new video series titled \"RAG from Scratch,\" aimed at explaining the Retrieval Augmented Generation (RAG) process, starting from basic principles and advancing to more complex topics. The series addresses the limitations of Large Language Models (LLMs), such as their inability to access private or very recent data not included in their pre-training. RAG is presented as a solution to integrate external data into LLMs, enhancing their functionality by expanding their context windows from thousands to many thousands of tokens.\n",
      "\n",
      "The RAG process involves three main stages: indexing external documents for easy retrieval, retrieving relevant documents based on a query, and using these documents to generate answers through an LLM. The series plans to cover these stages in detail, starting with the basics and progressing to more advanced techniques in future videos.\n",
      "\n",
      "Lance also provides a practical demonstration using a public repository, showcasing the installation of necessary packages, setting environment variables for Langsmith keys, and running a quick start code for RAG. This code involves loading and splitting a blog post, embedding and indexing the splits, defining a retriever, and setting up a chain to process an input question and generate an output using retrieved documents and an LLM. The demonstration highlights the practical application of RAG and sets the stage for deeper exploration in subsequent videos.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 5:\n",
      "In the third video of the \"RAG from Scratch\" series by Lance from LangChain, the focus is on the retrieval component of the RAG (Retrieval-Augmented Generation) process. Lance begins by recapping the previous videos where he discussed the indexing of documents, which simplifies the retrieval process. He explains that documents are split into smaller chunks, embedded into numerical representations, and stored in an index for easy retrieval.\n",
      "\n",
      "Lance illustrates the retrieval process using a simplified example of embedding documents into a three-dimensional space. The semantic content of the documents determines their position in this space, allowing documents with similar meanings to be located near each other. When a question is embedded into the same space, a similarity search is performed to find documents close to the question, based on their semantic relevance.\n",
      "\n",
      "He further explains the practical implementation of this concept using a parameter 'k', which determines the number of nearby documents to retrieve. In a demonstration, Lance sets k to 1 and retrieves a document relevant to the query \"what is task decomposition?\" He concludes by showing how this retrieval process can be easily implemented with a few lines of code and previews that the next topic in the series will be about the generation phase of RAG.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 6:\n",
      "In this video, Lance from LangChain introduces a technique called HIDE, which is part of their series on query translation in the context of Retrieval-Augmented Generation (RAG). The main goal of query translation is to transform an input question in a way that enhances the effectiveness of information retrieval. HIDE operates on the principle of converting questions into hypothetical documents to bridge the gap between the differing natures of questions and documents. Questions are typically short and may be poorly formulated, whereas documents are often lengthy and densely packed with information.\n",
      "\n",
      "Lance explains that by mapping questions into the \"document space\" through the creation of hypothetical documents, these can then be embedded and compared to actual documents in a high-dimensional space, potentially improving the retrieval process. He demonstrates this with a practical code walkthrough using OpenAI's tools. The process involves defining a prompt to generate a hypothetical document from a question, embedding this document, and then using it to retrieve relevant document sections from an indexed database.\n",
      "\n",
      "The retrieved documents are then used in conjunction with the original question in a RAG setup to generate an answer. Lance highlights that this method has shown promising results in certain domains and encourages experimentation with the document generation prompts to tailor the approach to specific needs. The technique aims to address some of the inherent challenges in document retrieval and is presented as both effective and easy to implement.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 7:\n",
      "Lance from LangChain presents a video on query translation and decomposition in the RAG from Scratch series. The focus is on enhancing retrieval by breaking down a user's input question into sub-questions, a technique popular in various research papers, including one from Google. This approach involves decomposing a complex question into simpler sub-problems, solving them sequentially, and using the solutions to address the overall query.\n",
      "\n",
      "For instance, the Google paper demonstrates this by decomposing the query \"think machine learning\" into sub-problems like \"think,\" \"think machine,\" and \"think machine learning,\" solving each to eventually concatenate the last letters of each word to form the answer \"KEG.\"\n",
      "\n",
      "Additionally, Lance discusses a related technique called IRCOT (Interleave Retrieval with Chain of Thought Reasoning), which dynamically combines retrieval with reasoning to solve decomposed sub-questions. This method uses answers from previous sub-questions to inform the solving of subsequent ones, building up to a final solution.\n",
      "\n",
      "Lance illustrates these concepts using a notebook, setting up a retriever and defining prompts to break down and solve a question about the components of an LLM-powered autonomous agent system. The process involves retrieving information and combining it with answers to prior questions to progressively solve each sub-question.\n",
      "\n",
      "He also mentions an alternative approach where sub-questions are answered independently and then concatenated, suitable for scenarios where the sub-questions are unrelated. This method, while simpler, does not utilize the answers from each question to inform the next, contrasting with the more integrated approach of IRCOT.\n",
      "\n",
      "In summary, the video explores advanced techniques in query decomposition and retrieval, demonstrating practical implementations and discussing their applications in solving complex queries effectively.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 8:\n",
      "In the 10th video of the Rack from Scratch series by Lance from LangChain, the focus is on routing, specifically following the process of query translation. Query translation involves modifying a question into a format more suitable for information retrieval. Routing then directs this modified question to the appropriate data source, which could vary among options like a vector store, relational database, or graph database.\n",
      "\n",
      "Lance explains two primary methods of routing: logical and semantic. Logical routing involves giving a large language model (LLM) knowledge about available data sources and allowing it to logically determine the most appropriate one for the query. This method is demonstrated with a toy example where a question is classified and routed to one of three document types (Python, JavaScript, or Golang docs) using a structured output model bound to the LLM.\n",
      "\n",
      "Semantic routing, on the other hand, involves embedding the question and various prompts, then selecting the most semantically similar prompt to route the query. This method is illustrated with an example where a question about black holes is matched to a physics prompt based on similarity scores.\n",
      "\n",
      "Throughout the video, Lance walks through the coding aspects of these routing methods, showing how to set up and utilize function schemas and structured outputs with the LLM to achieve precise routing. He encourages viewers to experiment with these routing techniques to enhance their data retrieval systems.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 9:\n",
      "Lance from LangShane introduces the second video in the \"Rack from Scratch\" series, focusing on the indexing component of Rack pipelines. He explains that indexing involves loading external documents into a retriever, which is designed to identify documents relevant to a given input question. This relevance is typically determined using numerical representations of documents, making them easier to compare than free-form text.\n",
      "\n",
      "He discusses historical and modern methods for creating these numerical representations. Initially, statistical methods like building sparse vectors based on word frequency were used. More recently, machine-learned embedding methods have been developed, which compress documents into fixed-length vectors that capture their semantic meanings. These vectors are then indexed, and similar methods are used to embed questions for comparison.\n",
      "\n",
      "Lance also provides a practical demonstration using a notebook. He shows how to compute the number of tokens in a question, specify an embedding model, and embed both questions and documents into vectors. He mentions using cosine similarity for comparison and illustrates how documents are split, embedded, and indexed in a vector store for retrieval. This process highlights the practical application of indexing and retrieval techniques using modern tools and methods.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 10:\n",
      "The text discusses the concept of multi-representation indexing, particularly focusing on its application in vector stores for enhancing document retrieval. This method is part of a broader series on building a Retrieval-Augmented Generation (RAG) system from scratch, with previous discussions covering topics like query translation, routing, and query construction for various databases.\n",
      "\n",
      "The main technique highlighted is inspired by a concept from a paper called \"Proposition Indexing,\" which suggests decoupling the raw documents from the retrieval units by using a process where documents are split and then distilled into propositions using a Large Language Model (LLM). These propositions serve as summaries optimized for retrieval, containing key ideas or keywords that make the documents easier to retrieve when matched with similar queries.\n",
      "\n",
      "In practice, the process involves creating summaries of documents, indexing these summaries in a vector store, and storing the original, full documents in a separate document store. When a query is made, the system retrieves the summary from the vector store, which then points to the full document in the document store, allowing the full document to be returned for detailed generation by the LLM.\n",
      "\n",
      "The text also includes a practical demonstration using blog posts about autonomous agents and human data quality. Summaries of these posts are created and indexed, and a query process is outlined showing how a query for specific terms like \"memory and agents\" leads to the retrieval of relevant documents based on the indexed summaries.\n",
      "\n",
      "This method is particularly beneficial for long-context LLMs, as it allows the model to access full documents, providing a richer context for generating responses, thereby enhancing the quality and relevance of the information retrieved.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 11:\n",
      "Lance from LangChain presents the fourth video in the RAG from Scratch series, focusing on the generation aspect of the RAG (Retrieval-Augmented Generation) process. The video builds on previous discussions about the basic workflow of RAG, which includes indexing, retrieval, and generation based on retrieved documents relevant to a posed question.\n",
      "\n",
      "In this installment, Lance explains how retrieved documents are integrated into the LLM (Large Language Model) context window for generating answers. The process involves splitting documents, embedding these splits into a vector store for easy searchability, and then using techniques like KNN (K-Nearest Neighbors) to find documents that closely match the numerical representation of a question.\n",
      "\n",
      "A significant part of the video is dedicated to explaining the creation of a prompt template, which acts as a structured format where retrieved documents and the question are organized to facilitate answer generation. This template is then filled with data to form a prompt that is processed by an LLM to produce an answer.\n",
      "\n",
      "Lance demonstrates this process using a coding example where he sets up a retriever, defines a prompt template, and connects it to an LLM through a chain defined in the LangChain Expression Language (LCEL). This setup allows for the automatic invocation of the chain to generate answers based on the context and question provided.\n",
      "\n",
      "Additionally, Lance introduces a more complex RAG prompt from their prompt hub and outlines an automated retrieval and generation process that simplifies the workflow by automatically populating the prompt with retrieved documents and generating answers.\n",
      "\n",
      "The video concludes with a promise of future videos that will explore more complex themes and address limitations of the current simple pipeline. This session effectively demonstrates the practical application and coding involved in setting up a RAG system using LangChain tools.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 12:\n",
      "The text discusses the concept of query translation, specifically focusing on the multi-query approach, as part of an advanced RAG (Retrieval-Augmented Generation) pipeline. The primary goal of query translation is to enhance document retrieval by reformulating a user's input question to reduce ambiguity and improve semantic alignment with relevant documents.\n",
      "\n",
      "The problem arises when user queries are ambiguous or poorly constructed, leading to ineffective retrieval of relevant documents. To address this, various strategies are employed, including query rewriting, which involves reframing the original question from different perspectives or breaking it down into sub-questions. This can make the query either less abstract or more abstract, depending on the approach.\n",
      "\n",
      "The multi-query method involves generating several differently worded versions of the original question to cover various perspectives. This increases the likelihood of the queries aligning well in the high-dimensional embedding space with the documents that contain the information sought. Each rephrased query is then used to retrieve documents independently, and the results are combined to improve the reliability of retrieval.\n",
      "\n",
      "In practice, the process involves setting up a notebook to index and split a blog post, define prompts for the LLM (Language Model) to generate multiple sub-questions, and then perform retrieval for each generated query. The retrieved documents from all queries are then unified to ensure comprehensive coverage of possible relevant information.\n",
      "\n",
      "The text concludes with a demonstration of running the multi-query setup, showing how the generated sub-questions lead to independent retrievals and how these are integrated in the final RAG prompt to answer the initial question. This approach not only enhances the retrieval process but also provides a framework for further exploration and refinement of query translation techniques in future videos.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 13:\n",
      "This text is a detailed explanation of a method called Rack Fusion, which is part of a series on query translation in advanced Rack pipelines. The main focus is on how Rack Fusion works to improve query retrieval by using a technique known as reciprocal rank fusion. The process begins with taking a user's question and breaking it down into several differently worded queries. These queries are then used to retrieve documents independently. The novelty of Rack Fusion lies in how it consolidates the results from these multiple retrievals into a single ranked list using reciprocal rank fusion, which aggregates the documents based on their ranks across the different lists.\n",
      "\n",
      "The video also includes a practical demonstration using a notebook where the method is implemented. API keys for a tool called LangSmith are set up, and the process is shown step-by-step, from generating multiple search queries based on a user input to retrieving documents for each query and finally applying the reciprocal rank fusion to rank these documents. The final output is a consolidated list of ranked documents, which is then used to generate a final answer to the user's question.\n",
      "\n",
      "This method is particularly useful for handling queries across different vector stores or when dealing with a large number of differently worded questions, as it allows for a more efficient and effective retrieval and ranking process.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Summarization text of the video Nr. 14:\n",
      "Lance from LangChain presents the 14th part of the RAG from Scratch series, focusing on an indexing approach called Colbert. The discussion begins with a recap of the series' flow, starting from query translation to optimize retrieval, routing to specific databases, and constructing queries in domain-specific languages for different database types. The series has previously covered various indexing methods, including multi-representation and hierarchical indexing.\n",
      "\n",
      "Colbert is introduced as an advanced embedding approach that addresses the limitations of compressing entire documents into single vectors. Instead, Colbert tokenizes documents and questions, creating an embedding for each token with positional weighting. The retrieval process involves computing the maximum similarity between each token in the question and all tokens in the document, summing these maximum values to score document relevance.\n",
      "\n",
      "Lance demonstrates using the Colbert approach with the Rakutui library, detailing the installation and usage of a pre-trained model to index and retrieve documents. He runs a test query on a document about Miyazaki, showing how Colbert can be integrated as a retriever within LangChain, suggesting its potential for handling longer documents and encouraging further experimentation with the approach.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# video order doesn't match as batch_size parameter is not supported (see llm above)\n",
    "output_content(map_reduce_outputs['intermediate_steps'], 'Summarization text of the video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original transcript of each single video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original transcript of the video Nr. 1:\n",
      "Hi, this is Lance from LangChain. This is the fourth video in our deep dive on query translation in the RAG from Scratch series, and we're going to be focused on step-back prompting. So query translation, as we said in some of the prior videos, kind of sits at the first stage of a RAG pipeline or flow, and the main aim is to take an input question and to translate it or modify it in such a way that it improves retrieval. Now, we talked through a few different ways to approach this problem. So one general approach involves rewriting a question, and we talked about two ways to do that, RAG fusion, multi-query, and again, this is really about taking a question and modifying it to capture a few different perspectives, which may improve the retrieval process. Now, another approach is to take a question and kind of make it less abstract, like break it down into sub-questions and then solve each of those independently. So that's what we saw with least to most prompting and a bunch of other variants kind of in that vein of sub-problem solving and consolidating those solutions into a final answer. Now, a different approach presented by, again, Google as well is step-back prompting. So step-back prompting takes the opposite approach where it tries to ask a more abstract question. So the paper talks a lot about using few-shot prompting to produce what they call the step-back or more abstract questions, and the way it does it is it provides a number of examples of step-back questions given your original question. So this is, for example, their per-prompt template. You're an expert in world knowledge. I ask you a question. Your response should be comprehensive, not contradictory with the following. And this is kind of where you provide your original and then step-back. So here's some example questions. So like, at your SAW, the creation of the region where the country is located, which region of the country is the county of Hertford related? Jensen-Dell is born in what country? What is Jensen-Dell's personal history? So that's going to be a more intuitive example. So it's like, you ask a very specific question about the country someone's born. The more abstract question is like, just give me the general history of this individual without worrying about that particular more specific question. So let's actually just walk through how this can be done in practice. So again, here's kind of like a diagram of the various approaches from less abstraction to more abstraction. Now, here is where we're formulating our prompt using a few of the few-shot examples from the paper. So again, like input, yeah, something about the police performing lawful arrests and what camp members of the police do. So like, it basically gives the model a few examples. We basically formulate this into a prompt. That's really all that's going on here. Again, we repeat this overall prompt, which we saw from the paper. Your next part of world knowledge, your task is to step back and paraphrase a question, generate a more generic step back question, which is easier to answer. Here's some examples. Very intuitive prompt. So, okay, let's start with the question, what is task composition for LLN agents? And we're going to say generate step back question. Okay, so this is pretty intuitive, right? What is the process of task composition? So like not worrying as much about agents, but what is the process of task composition in general? And then hopefully that can be independently retrieved. We can independently retrieve documents related to the step back question. And in addition, retrieve documents related to the actual question and combine those to produce kind of final answer. So that's really all that's going on. And here's the response template where we're plumbing in the step back context and our question context. And so what we're going to do here is we're going to take our input question and perform retrieval on that. We're also going to generate our step back question and perform retrieval on that. We're going to plumb those into the prompt as here's our basically our prompt keys, normal question, step back question, and our overall question. Again, we formulate those as a dict. We plumb those into our response prompt. And then we go ahead and attempt to answer our overall question. So we're going to run that. That's running. And OK, we have our answer. Now I want to hop over to Langsmith and attempt to show you kind of what that looked like under the hood. So let's see. Let's go into each of these steps. So here was our prompt, right? You're an expert in world knowledge. You're tasked to step back and paraphrase a question. So here are a few shot prompts. And this was our this was our step back question. So what is the process of task composition? Good. From the input, what is the task composition for LLM agents? We perform retrieval on both. What is the process of task composition? And what is task composition for LLM agents? We perform both retrievals. We then populate our prompt with both. Original question, answer. And then here is the context retrieved from both the question and the step back question. Here is our final answer. So, again, this is kind of a nice technique. Probably depends on a lot of the types of like the type of domain you want to perform retrieval on. But in some domains where, for example, there's a lot of kind of conceptual knowledge that underpins questions you expect users to ask, this step back approach could be really convenient to automatically formulate a higher level question to, for example, try to improve retrieval. I can imagine if you're working with like kind of textbooks or like technical documentation where you've made independent chapters focused on more high level kind of like concepts and then other chapters on like more detailed like implementations. This kind of like step back approach and independent retrieval could be really helpful. Thanks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 2:\n",
      "Hi, this is Lance from LangChain. This is the 11th part of our RAG from Scratch video series, focused on query construction. So we previously talked through query translation, which is the process of taking a question and converting it or translating it into a question that's better optimized for retrieval. Then we talked about routing, which is the process of taking that question and routing it to the right source, be it a given vector store, GraphDB, or SQLDB, for example. Now we're going to talk about the process of query construction, which is basically taking natural language and converting it into particular domain-specific language for one of these sources. Now we're going to talk specifically about the process of going from natural language to metadata filters for vector stores. The problem statement is basically this. Let's imagine we had an index of LangChain video transcripts. You might want to ask a question, find me videos on chat LangChain published after 2024, for example. The process of query structuring basically converts this natural language question into a structured query that can be applied to the metadata filters on your vector store. So most vector stores will have some kind of metadata filters that can do kind of structured querying on top of the chunks that are indexed. So for example, this type of query will retrieve all chunks that talk about the topic of chat LangChain published after the date 2024. That's kind of the problem statement. And to do this, we're going to use function calling. In this case, you can use, for example, OpenAI or other providers to do that. And what we're going to do is, at a high level, take the metadata fields that are present in our vector store and provide them to the model as kind of information. And the model then can take those and produce queries that adhere to the schema provided. And then we can parse those out to a structured object, like a pedantic object, which can then be used in search. So that's kind of the problem statement. And let's actually walk through code. So here's our notebook, which we've kind of gone through previously. And I'll just show you as an example. Let's take an example YouTube video and let's look at the metadata that you get with the transcript. So you can see you get stuff like description, URL, yeah, publish date, length, things like that. Now, let's say we had an index that had, basically, it had a number of different metadata fields and filters that allowed us to do range filtering on, like, view count, publication date, video length, or unstructured search on contents and title. So those are kind of like the, imagine we had an index that had those kind of filters available to us. What we can do is capture that information about the available filters in an object. So we're calling that this tutorial search object, kind of encapsulates that information about the available searches that we can do. And so we basically enumerate it here, content search and title search are semantic searches that can be done over those fields. And then these filters, then, are various types of structured searches we can do on, like, the length, the view count, and so forth. And so we can just kind of build that object. Now, we can set this up really easily with a basic simple prompt that says, you know, you're an expert, can bring natural language into database queries, you have access to the database tutorial videos, given a question, return a database query optimized for retrieval. So that's kind of it. Now, here's the key point, though. When you call this LLM with structured output, you're binding this pedantic object, which contains all the information about our index, to the LLM, which is exactly what we talked about previously. It's really this process right here. You're taking this object, you're converting it into a function schema, for example, OpenAI, you're binding that to your model, and then you're going to be able to get a structured object out versus JSON string from a natural language question, which can then be parsed into a pedantic object, which you get out. So that's really the flow. And it's taking advantage of function calling, as we said. So if we go back down, we set up our query analyzer chain right here. Now, let's try to run that just on a purely semantic input. So, rag from scratch. Let's run that. And you can see this just does like a content search and a title search. That's exactly what we would expect. Now, if we pass a question that includes like a date filter, let's just see if that would work. And there we go. So you kind of still get that semantic search, but you also get search over, for example, published date, earliest and latest published date, kind of as you would expect. Let's try another one here. So videos focus on the topic of chat landing chain. They're published before 2024. This is just kind of a rewrite of this question in a slightly different way, using a different date filter. And then you can see we get content search, title search, and then we can get kind of a date search. So this is a very general strategy that can be applied kind of broadly to different kinds of querying you want to do. It's really the process of going from an unstructured input to a structured query object out, following an arbitrary schema that you provide. And so, as noted, really this whole thing we created here, this tutorial search, is based upon the specifics of our vector store of interest. And if you want to learn more about this, I linked to some documentation here that talks a lot about different types of integrations we have with different vector store providers to do exactly this. So it's a very useful trick. It allows you to do kind of query, say, metadata filtering on the fly from a natural language question. It's a very convenient trick that works with many different vector DBs. So I encourage you to play with it. Thanks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 3:\n",
      "Hi, this is Lance from LangChain. This is the 13th part of our RAG from Scratch series, focused on a technique called Raptor. So Raptor sits within kind of an array of different indexing techniques that can be applied on vector stores. We just talked about multi-representation indexing. We I provided a link to a video that's very good, talking about the different means of chunking. So I encourage you to look at that. And we're going to talk today about a technique called Raptor, which you can kind of think of it as a technique for hierarchical indexing. So the high-level intuition is this. Some questions require very detailed information from a corpus to answer, like pertain to a single document or single chunk. So like we can call those low-level questions. Some questions require consolidation across kind of broad swaths of a document. So across like many documents or many chunks within a document, and you can call those like higher-level questions. And so there's kind of this challenge in retrieval in that typically we do like k-nearest-neighbors retrieval like we've been talking about. You're fishing out some number of chunks, but what if you have a question that requires information across like five, six, you know, or a number of different chunks, which may exceed, you know, the k-parameter in your retrieval. So again, when you typically do retrieval, you might set a k-parameter of three, which means you're retrieving three chunks from your vector store. And maybe you have a very high-level question that could benefit from information across more than three. So this technique called Raptor is basically a way to build a hierarchical index of document summaries. And the intuition is this. You start with a set of documents as your leafs here on the left, you cluster them, and then you summarize each cluster. So each cluster of similar documents will consult information from across your context, which is, you know, your context could be a bunch of different splits or could even be across a bunch of different documents. You're basically capturing similar ones and you're consulting the information across them in a summary. And here's the interesting thing. You do that recursively until either you hit like a limit or you end up with one single cluster. You end up with a high-level summary of all of your documents. And what the paper shows is that if you basically just collapse all these and index them together as a big pool, you end up with a really nice array of chunks that span the abstraction hierarchy. Like you have a bunch of chunks from individual documents that are just like more detailed chunks pertaining to that, you know, single document. But you also have chunks from these summaries, or I would say like, you know, maybe not chunks, but in this case, the summary is like a distillation. So, you know, raw chunks on the left that represent your leafs are kind of like the rawest form of information, either raw chunks or raw documents. And then you have these higher-level summaries, which are all indexed together. So if you have higher-level questions, they should basically be more similar in semantic search, for example, to these higher-level summary chunks. If you have lower-level questions, then they'll retrieve these more lower-level chunks. And so you have better semantic coverage across like the abstraction hierarchy of question types. That's the intuition. They do a bunch of nice studies to show that this works pretty well. I actually did a deep dive video just on this, which I linked below. I did want to cover it briefly, just at a very high level. So let's actually just do kind of a code walkthrough. And I've added it to this Rack from Scratch course notebook, but I link over to my deep dive video as well as the paper and the full code notebook, which is already checked in and is discussed at more length in the deep dive. The technique is a little bit detailed, so I only want to give you very high-levels kind of overview here. And you can look at the deep dive video if you want to go in more depth. Again, we talked through this abstraction hierarchy. I applied this to a large set of lang chain documents. So this is me loading basically all of our lang chain expressions. So this is on the order of 30 documents. You can see I do a histogram here of the token counts per document. Some are pretty big. Most are fairly small, less than 4,000 tokens. And what I did is I indexed all of them individually. So all those raw documents you can kind of imagine are here on the left. And then I do embedding, I do clustering, summarization, and I do that recursively until I end up with, in this case, I believe I only set like three levels of recursion. And then I save them all in my vector store. So that's like the high-level idea. I'm applying this Raptor technique to a whole bunch of lang chain documents that have fairly large number of tokens. So I do that. And, yeah, I use, actually, I use both Claude as well as OpenAI here. This talks through the clustering method that they use, which is pretty interesting. You can kind of dig into that on your own if you're really interested. This is a lot of their code, which I cite accordingly. This is basically implementing the clustering method that they use. And this is just simply the document embedding stage. This is like basically embedding and clustering. That's really it. This is some text formatting, summarizing of the clusters right here. And then this is just running that whole process recursively. That's really it. This is tree building. So basically I have the raw docs. Let's just go back and look at doc texts. So this should be all my raw documents. So that's right, you can see it here. Doc text is basically just the text and all those lang chain documents that I pulled. So I run this process on them right here. So this is that recursive embedding cluster, basically runs and produces that tree. Here's the results. This is me just going through the results and basically adding the result text to this list of texts. OK, so here's what I do. This leaf text is all the raw documents. And I'm appending to that all the summaries. That's all that's going on. And then I'm indexing them all together. That's the key point. Rag chain, and there you have it. That's really all you do. So anyway, I encourage you to look at this in depth. It's a pretty interesting technique. It works well with long contexts. So for example, one of the arguments I made is that it's kind of a nice approach to consult information across like a span of large documents. Like in this particular case, my individual documents were lang chain expression language docs, each being somewhere in the order of, in this case, most of them are less than 4,000 tokens, some pretty big. But I index them all. I cluster them without any splits, embed them, cluster them, build this tree, and go from there. And it all works because we now have LLMs that can go out to 200,000 up to a million tokens in context. So you can actually just do this process for a big swath of documents in place without any splitting. It's a pretty nice approach. So I encourage you to think about it, look at it, watch the deep dive video if you really want to go deeper on this. Thanks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 4:\n",
      "Hi, this is Lance from Langchain. We're starting a new series called RAG from Scratch that's going to walk through some of the basic principles for RAG and kind of build up to advanced topics. So one of the main motivations for RAG is simply that LLMs haven't seen all of the data that you may care about. So like private data or very recent data would not be included in the pre-training runs for these LLMs. And you can see here on the graph on the x-axis that the number of tokens that they're pre-trained on, which is of course very large, but of course it's still always going to be limited relative to private data that you care about, or for example, recent data. But there's another interesting consideration is that LLMs have context windows that are actually getting increasingly large. So going from thousands of tokens to many thousands of tokens, which represents dozens of pages up to hundreds of pages, we can fit information into them from external sources. And a way to think about this is LLMs are kind of a kernel of a new kind of operating system and connecting them to external data is kind of a very central capability in the development of this kind of new emergent operating system. Retrieval Augmented Generation, or RAG, is a very popular kind of general paradigm for doing this, which typically involves three stages. So the first stage is indexing some external documents such that they can be easily retrieved based on an input query. So for example, we ask a question, we retrieve documents that are relevant to that question, we feed those documents into an LLM in the final generation stage to produce an answer that's grounded in those retrieved documents. Now we're starting from scratch, but we're going to kind of build up to this broader view of RAG. You can see here, there's a lot of interesting methods and tricks that kind of fan out from those three basic components of indexing, retrieval, and generation. And future videos are actually going to walk through those in detail. We're going to try to keep each video pretty short, like five minutes, but we're going to spend a lot of time on some of those more advanced topics. First, over the next three videos, I'll just be laying out the very basic kind of ideas behind indexing, retrieval, and generation, and then we'll kind of build beyond that into those more advanced themes. And now I want to show just a quick code walkthrough, because we want to make these videos also a little bit interactive. So right here, and this repo will be shared, it's public. I have a notebook open, and I've just basically installed a few packages. And I've set a few environment variables for my Langsmith keys, which I personally do recommend. It's really useful for tracing observability, particularly when you're building RAG pipelines. So what I'm going to show here is the code for our RAG quick start, which is linked here. And I'm going to run this, but I'm then going to kind of walk through everything that's going on. So actually, if we think back to our diagram, all we're doing here is we're loading documents. In this case, I'm loading a blog post. We're then splitting them, and we'll talk about that in future short videos on why splitting is important, but just for now, recognize we're splitting them or setting a chunk size of 1,000 characters. So we're splitting up our documents. Every split is embedded and indexed into this vector store, so we said we picked open AI embeddings. We're using Chromas, our vector store, which runs locally. And now we define this retriever. We then have defined a prompt for RAG. We've defined our LLM. We've done some minor document processing. We set up this chain, which will basically take our input question, run our retriever to fetch relevant documents, put the retrieved documents and our question into our prompt, pass it to LLM, format the output as a string, and we can see here's our output. Now we can open up LangSmith, and we can actually see how this ran. So here was our question, and here's our output, and we can actually look. Here's our retriever. Here's our retrieved documents, so that's pretty nice. And ultimately, here was the prompt that we actually passed into the LLM. You're an assistant for QA tasks. Use the following pieces of retrieved content to answer the question. Here's our question, and then here's all the content that we retrieved, and that drills in our answer. So this just gives a very general overview of how RAG works, and in future short videos, we're going to break down each of these pieces in a lot more detail. Thanks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 5:\n",
      "Hi, this is Lance from LangChain, and this is the third video in our series, RAG from Scratch, building up a lot of the motivations for RAG from the very basic components. So we're going to be talking about retrieval today. In the last two short videos, I outlined indexing and gave kind of an overview of this flow, which starts with indexing of our documents, retrieval of documents relevant to our question, and then generation of answers based on the retrieved documents. And so we saw that the indexing process basically makes documents easy to retrieve. And it goes through a flow that basically looks like you take your documents, you split them in some way into these smaller chunks that can be easily embedded. Those embeddings are then numerical representations of those documents that are easily searchable, and they're stored in an index. When given a question that's also embedded, the index performs a similarity search and returns splits that are relevant to the question. Now if we dig a little bit more under the hood, we can think about it like this. If we take a document and embed it, let's imagine that embedding just had three dimensions. So each document is projected into some point in this 3D space. Now the point is that the location in space is determined by the semantic meaning or content in that document. So to follow that then, documents in similar locations in space contain similar semantic information. And this very simple idea is really the cornerstone for a lot of search and retrieval methods that you'll see with modern vector stores. So in particular, we take our documents, we embed them into this, in this case, a toy 3D space. We take our question, do the same. We can then do a search, like a local neighborhood search you can think about in this 3D space around our question to say, hey, what documents are nearby? And these nearby neighbors are then retrieved because they can, they have similar semantics relative to our question. And that's really what's going on here. So again, we took our documents, we split them, we embed them, and now they exist in this high dimensional space. We've taken our question, embedded it, projected in that same space, and we just do a search around the question for nearby documents and grab ones that are close. And we can pick some number. We can say we want one or two or three or N documents close to my question in this embedding space. And there's a lot of really interesting methods that implement this very effectively. I link one here. And we have a lot of really nice integrations to play with this general idea. So many different embedding models, many different indexes, lots of document loaders, and lots of splitters that can be kind of recombined to test different ways of doing this kind of indexing or retrieval. So now I'll show a bit of a code walkthrough. So here we defined, we kind of walked through this previously. This is our notebook. We've installed a few packages. We've set a few environment variables using Langsmith. And we showed this previously. This is just an overview showing how to run RAG, like kind of end to end. In the last short talk, we went through indexing. And what I'm going to do very simply is I'm just going to reload our documents. So now I have our documents. I'm going to resplit them. And we saw before how we can build our index. Now here, let's actually do the same thing. But in the slides, we actually showed kind of that notion of search in that 3D space. And a nice parameter to think about in building your retriever is k. So k tells you the number of nearby neighbors to fetch when you do that retrieval process. And we talked about, you know, in that 3D space, do I want one nearby neighbor or two or three? So here, we can specify k equals 1, for example. Now we're building our index. So we're taking every split, embedding it, storing it. Now what's nice is I ask a question, what is task decomposition? This is related to the blog post. And I'm going to run get relevant documents. So I run that. And now how many documents do I get back? I get one, as expected, based on k equals 1. So this retrieved document should be related to my question. Now I can go to LangSmith. And we can open it up. And we can look at our retriever. And we can see here is our question. Here's the one document we got back. And OK, so that makes sense. This document pertains to task decomposition in particular. And it kind of lays out a number of different approaches that can be used to do that. This all kind of makes sense. And this shows kind of in practice how you can implement this kind of knn or k nearest neighbor search really easily just using a few lines of code. And next, we're going to talk about generation. Thanks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 6:\n",
      "Hi, this is Lance from LangChain. This is the fifth video focused on query translation in our RAG from Scratch series. We're going to be talking about a technique called HIDE. So again, query translation sits kind of at the front of the overall RAG flow. And the objective is to take an input question and translate it in some way that improves retrieval. Now, HIDE is an interesting approach that takes advantage of a very simple idea. The basic RAG flow takes a question and embeds it, takes a document and embeds it, and looks for similarity between an embedded document and an embedded question. But questions and documents are very different text objects. So documents can be like very large chunks taken from dense publications or other sources, whereas questions are short, kind of terse, potentially ill-worded from users. And the intuition behind HIDE is take questions and map them into document space using a hypothetical document or by generating a hypothetical document. That's the basic intuition. And the idea kind of shown here visually is that in principle for certain cases, the hypothetical document is closer to a desired document you actually want to retrieve in this high-dimensional embedding space than the sparse raw input question itself. So again, it's just kind of means of translating raw questions into these hypothetical documents that are then better suited for retrieval. So let's actually do a code walkthrough to see how this works. And it's actually pretty easy to implement, which is really nice. So first, we're just starting with a prompt. And we're using the same notebook that we used for prior videos. We have a blog post on agents already indexed. So what we're going to do is define a prompt to generate a hypothetical document. In this case, we'll say write a paper passage to answer a given question. So let's just run this and see what happens. Again, we're taking our prompt, piping it to OpenAI, check GPT, and then using string output parser. And so here's a hypothetical document section related to our question. And this is derived, of course, from LLM's kind of embedded kind of world knowledge, which is a sane place to generate hypothetical documents. Now, let's now take that hypothetical document. And basically, we're going to pipe that into a retriever. So this means we're going to fetch documents from our index related to this hypothetical document that's been embedded. And you can see we get a few retrieved chunks that are related to this hypothetical document. That's all we've done. And then let's take the final step where we take those retrieved documents here, which we defined, and our question. And we're going to pipe that into this rag prompt. And then we're going to run our kind of rag chain right here, which you've seen before. And we get our answer. So that's really it. We can go to LangSmith, and we can actually look at what happened. So here, for example, this was our final rag prompt. Answer the following question based on this context. And here is the retrieved documents that we passed in. So that part's kind of straightforward. We can also look at, okay, this is our retrieval. Okay, no, this is actually what we generated, a hypothetical document here. Okay, so this is our hypothetical document. So we've run chat open AI. We generated this passage, which is our hypothetical document. And then we've run retrieval here. So this is basically showing hypothetical document generation followed by retrieval. So, again, here was our passage, which we passed in, and then here's our retrieved documents from the retriever, which are related to the passage content. So, again, in this particular index case, it's possible that the input question was sufficient to retrieve these documents. In fact, given prior examples, I know that some of these same documents are indeed retrieved just from the raw question. But in other contexts, that may not be the case. So folks have reported nice performance using HIDE for certain domains. And the really convenient thing is that you can take this document generation prompt, you can tune this arbitrarily for your domain of interest. So it's absolutely worth experimenting with. It's a neat approach that can overcome some of the challenges with retrieval. Thanks very much.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 7:\n",
      "Hi, this is Lance from LangChain. This is our third video focused on query translation in the RAG from Scratch series, and we're going to be talking about decomposition. So query translation in general is a set of approaches that sits kind of towards the front of this overall RAG pipeline, and the objective is to modify, rewrite, or otherwise decompose an input question from a user in order to improve retrieval. So we kind of talked through some of these approaches previously, in particular various ways to do query rewriting, like RAG fusion and multi-query. There's a separate set of techniques that have become pretty popular and are really interesting for certain problems, which you might call like breaking down or decomposing an input question into a set of sub-questions. So some of the papers here that are pretty cool are, for example, this work from Google, and the objective really is first to take an input question and decompose it into a set of sub-problems. So this particular example from the paper was the problem of last letter concatenation, and so it took the input question of three words, think machine learning, and broke it down into three sub-problems, think, think machine, think machine learning, as the third sub-problem. And then you can see in this bottom panel, it solves each one individually. So it shows, for example, in green, solving the problem of think machine, where you concatenate the last letter of K with the last letter of machine, or the last letter of think K, lesser of machine E, concatenate those to KE, and then for the overall problem, taking that solution and then basically building on it to get the overall solution of KEG. So that's kind of one concept of decomposing into sub-problems, solving them sequentially. Now a related work called IRCOT, or interleave retrieval, combines retrieval with chain of thought reasoning. And so you can kind of put these together into one approach, which you can think of as kind of dynamically retrieval to solve a set of sub-problems, kind of that retrieval kind of interleaving with chain of thought, as noted in the second paper, and a set of decomposed questions based on your initial question from the first work from Google. So really the idea here is we're taking one sub-question, we're answering it, we're taking that answer and using it to help answer the second sub-question and so forth. So let's actually just walk through this in code to show how this might work. So this is the notebook we've been working with from some of the other videos. You can see we already have a retriever defined up here at the top. And what we're going to do is we're first going to find a prompt that's basically going to say, given an input question, let's break it down to set up sub-problems or sub-questions which can be solved individually. So we can do that, and this blog post is focused on agents. So let's ask a question about what are the main components of an LLM-powered autonomous agent system? So let's run this and see what the decomposed questions are. So you can see the decomposed questions are what is LLM technology, how does it work, what are specific components, and then how the components interact. So it's kind of a sane way to kind of break down this problem into a few sub-problems which you might attack individually. Now here's where we define a prompt that very simply is going to take our question, we'll take any prior questions we've answered, and we'll take our retrieval and basically just combine them. And we can define this very simple chain. Actually let's go back and make sure retriever is defined up at the top. So now we are building our retriever. Good, we have that now. So we can go back down here and let's run this. So now we are running, and what's happening is we're trying to solve each of these questions individually, using retrieval and using any prior question answers. So okay, very good, looks like that's been done, and we can see here's our answer. Now let's go over to Languages and actually see what happened under the hood. So here's what's kind of interesting and helpful to see. For the first question, so here's our first one, it looks like it just does retrieval, which is what we expect, and then it uses that to answer this initial question. Now for the second question, it should be a little bit more interesting, because if you look at our prompt, here's our question. Now here is our background available question-answer pair, so this was the question-answer pair from the first question, which we add to our prompt, and then here's the retrieval for this particular question. So we're kind of building up the solution, because we're pending the question-answer pair from question one. And then likewise, with question three, it should combine all of that, so we can look at here, here's our question, here's question one, here's question two, great. Now here's additional retrieval related to this particular question, and we get our final answer. So that's like a really nice way you can kind of build up solutions, using this kind of interleaved retrieval, and concatenating prior question-answer pairs. I do want to mention very briefly that we can also take a different approach, where we can just answer these all individually, and then just concatenate all those answers to produce a final answer, and I'll show that really quickly here. It's like a little bit less interesting, maybe, because you're not using answers from each question to inform the next one, you're just answering them all in parallel. This might be better for cases where it's not really like a sub-question decomposition, but maybe it's like a set of several independent questions whose answers don't depend on each other. That might be relevant for some problems. And we can go ahead and run, okay, so this ran as well, we can look at our trace, and in this case, yeah, we can see that this actually just kind of concatenates all of our QA pairs to produce the final answer. So this gives you a sense for how you can use the query decomposition, employ ideas from two different papers that are pretty cool. Thanks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 8:\n",
      "Hi, this is Lance from LangChain. This is the 10th video in our Rack from Scratch series focused on routing. We talked through query translation, which is the process of taking a question and translating in some way. It could be decomposing it, using step-back prompting or otherwise. But the idea here was take our question, change it into a form that's better suited for retrieval. Now, routing is the next step, which is basically routing that potentially decomposed question to the right source. In many cases, that could be a different database. Let's say in this toy example, we have a vector store relational DB and a graph DB, that what we'll redo with routing is we simply route the question based upon the content of the question to the relevant data source. There's a few different ways to do that. One is what we call logical routing. In this case, we basically give an LLM knowledge of the various data sources that we have at our disposal, and we let the LLM reason about which one to apply the question to. It's like the LLM is applying some logic to determine which data source, for example, to use. Alternatively, you can use semantic routing, which is where we take a question, we embed it, and for example, we embed prompts. We then compute the similarity between our question and those prompts, and then we choose a prompt based upon the similarity. The general idea is, in our diagram, we talk about routing to, for example, different database, but it can be very general, it can be routing to different prompt, it can be really arbitrarily taking this question and sending it in different places, be it different prompts, be it different vector stores. Let's walk through the code a little bit. You can see just like before, we've done a few pip installs, we've set up Langsmith, and let's talk through logical routing first. In this toy example, let's say we had, for example, three different docs, like we had Python docs, we had JS docs, we had Golang docs. What we want to do is take a question, route it to one of those three. What we're actually doing is we're setting up a data model, which is basically going to be bound to our LLM, and allow the LLM to output one of these three options as a structured object. You're really thinking about this as a classification. Classification plus function calling to produce a structured output, which is constrained to these three possibilities. The way we do that is, let's just zoom in here a little bit. We can define a structured object we want to get out from our LLM. Like in this case, we want, for example, one of these three data sources to be output. We can take this and we can actually convert it into OpenAI, for example, function schema. Then we actually pass that in and bind it to our LLM. What happens is, we ask a question, our LLM invokes this function on the output to produce an output that adheres to the schema that we specify. In this case, for example, we output in this toy example, let's say we wanted an output to be data source, VectorStore or SQL database. The output will contain a data source object and it'll be one of the options we specify as a JSON string. We also instantiate a parser from this object to parse that JSON string to an output like a pedantic object, for example. That's just one toy example. Let's show one up here. In this case, again, we had our three doc sources. We bind that to our LLM. You can see we do with structured output, basically under the hood, that's taking that object definition, turning into function schema and binding that function schema to our LLM. We call our prompt, you're an expert at routing a user question based on programming language that you're referring to. Let's define our router here. Now what we're going to do is, we'll ask a question that is Python code. We'll call that and now it's done. You can see the object we get out is indeed, it's a route query object. It's exactly and here's this data model we've set up. In this case, it's correct. It's calling this Python docs. We can extract that right here as a string. Now, once we have this, you can really easily set up a route. This could be like our full chain where we take this router, which is defined here, and then this choose route function can basically take that output and do something with it. For example, if Python docs, this could then apply the question to like a retriever full of Python information or JS same thing. This is where you would hook basically that question up to different chains that are like retriever chain one for Python, retriever chain two for JS and so forth. This is the routing mechanism, but this is really doing the heavy lifting of taking an input question and turning it into a structured object that restricts the output to one of a few output types that we care about in our routing problem. That's really the way this all hooks together. Now, semantic routing is actually maybe a little bit more straightforward based on what we've seen previously. In that case, let's say we have two prompts. We have a physics prompt, we have a math prompt. We can embed those prompts. No problem, we do that here. Now, let's say we have an input question from a user, like in this case, what is a black hole? We pass that through. We then apply this run of a Lambda function, which is defined right here. What we're doing here is we're embedding the question. We're computing similarity between the question and the prompts. We're taking the most similar. And then we're basically choosing the prompt based on that similarity. And you can see, let's run that and try it out. And we're using the physics prompt and there we go, black holes region and space. So that just shows you kind of how you can use semantic routing to basically embed a question, embed, for example, various prompts, pick the prompt based on semantic similarity. So that really gives you just two ways to do routing. One is logical routing with function calling. It can be used very generally. In this case, we applied it to like different coding languages, but imagine these could be swapped out for like, you know, my Python, my like vector store versus my graph DB versus my relational DB. And you could just very simply have some description of what each is. And, you know, then not only will the LLM do reasoning, but it'll also return an object that can be parsed very cleanly to produce like one of a few very specific types, which then you can reason over like we did here in your routing function. So that kind of gives you the general idea. And these are really very useful tools and I encourage you to experiment with them. Thanks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 9:\n",
      "Hi, this is Lance from LangShane. This is the second video in our series Rack from Scratch focused on indexing. So in the past video you saw the main kind of overall components of Rack pipelines, indexing, retrieval, and generation. And here we're going to kind of deep dive on indexing and give just a quick overview of it. So the first aspect of indexing is we have some external documents that we actually want to load and put into what we're trying to call a retriever. And the goal of this retriever is simply, given an input question, I want to fish out documents that are related to my question in some way. Now the way to establish that relationship or relevance or similarity is typically done using some kind of numerical representation of documents. And the reason is that it's very easy to compare vectors, for example, with numbers relative to just free-form text. And so a lot of approaches have been developed over the years to take text documents and compress them down into a numerical representation that then can be very easily searched. Now there's a few ways to do that. So Google and others came up with many interesting statistical methods where you take a document, you look at the frequency of words, and you build what they call sparse vectors such that the vector locations are a large vocabulary of possible words. Each value represents the number of occurrences of that particular word. And it's sparse because there's, of course, many zeros. It's a very large vocabulary relative to what's present in the document. And there's very good search methods over this type of numerical representation. Now a bit more recently, embedding methods that are machine-learned. So you take a document and you build a compressed fixed-length representation of that document have been developed with correspondingly very strong search methods over embeddings. So the intuition here is that we take documents and we typically split them because embedding models actually have limited context windows. So on the order of maybe 512 tokens up to 8000 tokens or beyond, but they're not infinitely large. So documents are split and each document is compressed into a vector. And that vector captures a semantic meaning of the document itself. The vectors are indexed. Questions can be embedded in exactly the same way. And then a numerical kind of comparison in some form, using different types of methods, can be performed on these vectors to fish out relevant documents relative to my question. And let's just do a quick code walkthrough on some of these points. So I have my notebook here. I've installed here. Now I've set a few API keys for Langsmith, which are very useful for tracing, which we'll see shortly. Previously I walked through this kind of quick start that just showed overall how to lay out these rag pipelines. And here what I'll do is I'll deep dive a little bit more on indexing and I'm going to take a question and a document. And first I'm just going to compute the number of tokens in, for example, the question. And this is interesting because embedding models and LLMs more generally operate on tokens. And so it's kind of nice to understand how large the documents are that I'm trying to feed in. In this case, it's obviously a very small, in this case, question. Now I'm going to specify OpenAI embeddings. I specify an embedding model here and I just say embed, embed query. I can pass my question, my document. And what you can see here is that runs. And this is mapped to now a vector of length 1536. And that fixed length vector representation will be computed for both documents and really for any documents. So you're always kind of computing this fixed length vector that encodes the semantics of the text that you've passed. Now I can do things like cosine similarity to compare them. And as we'll see here I can load some documents. This is just like we saw previously. I can split them and I can index them here, just like we did before. But we can see under the hood, really what we're doing is we're taking each split, we're embedding it using OpenAI embeddings into this vector representation and that's stored with a link to the raw document itself in our vector store. And next we'll see how to actually do retrieval using this vector store.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 10:\n",
      "I'm going to talk about indexing and multi-representation indexing in particular for the 12th part of our RAG from Scratch series here. So we previously talked about a few different major areas. We talked about query translation, which takes a question and translates it in some way to optimize for retrieval. We talked about routing, which is the process of taking a question, routing it to the right data source, be it a vector store, GraphDB, SQLDB. We talked about query construction. We dug into basically query construction for vector stores, but of course there's also text to SQL, text to Cypher. So now we're going to talk about indexing a bit. In particular, we're going to talk about indexing techniques for vector stores. And I want to highlight one particular method today called multi-representation indexing. So the high-level idea here is derived a bit from a paper called Proposition Indexing, which kind of makes a simple observation. You can think about decoupling raw documents and the units you use for retrieval. So in the typical case, you take a document, you split it up in some way to index it, and then you embed the split directly. This paper talks about actually taking a document, splitting it in some way, but then using an LLM to produce what they call a proposition, which you can think of as like kind of a distillation of that split. So it's kind of using an LLM to modify that split in some way, to distill it or make it like a crisper summary, so to speak, that's better optimized for retrieval. So that's kind of one piece of intuition. So we've actually taken that idea and we've kind of built on it a bit in kind of a really nice way that I think is very well suited actually for long-context LLMs. So the idea is pretty simple. You take a document and you actually distill it or create a proposition like they show in the prior paper. I kind of typically think of this as just produce a summary of the document and you embed that summary. So that summary is meant to be optimized for retrieval, so it might contain a bunch of keywords from the document or like the big ideas such that when you embed the summary, you embed a question, you do search, you basically can find that document based upon this highly optimized summary for retrieval. So that's kind of represented here in your vector store, but here's the catch. You independently store the raw document in a doc store. And when you basically retrieve the summary in the vector store, you return the full document for the LLM to perform generation. And this is a nice trick because at generation time, now with long-context LLMs, for example, the LLM can handle that entire document. You don't need to worry about splitting it or anything. You just simply use the summary to create a really nice representation for fishing out that full doc, use that full doc in generation. There might be a lot of reasons you want to do that. You want to make sure the LLM has the full context to actually answer the question. So that's the big idea. Nice trick. And let's walk through some code here. So we have a notebook all set up just like before. We've done some PIP installs, set some API keys here for LangSmith, kind of here's a diagram. Now let me show an example. Let's just load two different blog posts. One is about agents. One is about human data quality. And what we're going to do is let's create a summary of each of those. So this is kind of the first step of that process where we're going from like the raw documents to summaries. Let's just have a look and make sure those ran. So okay, cool. So the first doc discusses building autonomous agents. The second doc contains the importance of high quality human data and training. So that's pretty nice. We have our summaries. Now we're going to go through a process that's pretty simple. First we define a vector store that's going to index those summaries. Then we're going to define what we call like our document storage is going to store the full documents. Okay. So this multi-vector retriever kind of just pulls those two things together. We basically add our doc store. We add this byte store is basically the full document store. The vector store is our vector store. And now this ID is what we're going to use to reference between the chunks or the summaries and the full documents. That's really it. So now for every document, we'll define a new doc ID. And then we're basically going to like take our summary documents and we're going to extract for each of our summaries, we're going to get the associated doc ID. So there we go. So let's go ahead and do that. So we have our summary docs, which we add to the vector store. We have our full documents, our doc IDs and the full raw documents, which are added to our doc store. And then let's just do a query, a vector store, like a similarity search on our vector store. So memory and agents, and we can see, okay, so we can extract, you know, from the summaries, we can get, for example, the summary that pertains to agents. That's a good thing. Now let's go ahead and run a query, get relevant documents on our retriever, which basically combines the summaries, which we use for retrieval, then the doc store, which we use to get the full doc back. So we're going to apply our query. We're going to basically run this. And here's the key point. We've gotten back the entire article. And we can actually, if you want to look at the whole thing, we can just go ahead and do this. Here we go. So this is the entire article that we get back from that search. So it's a pretty nice trick. Again, we query with just memory and agents, and we can kind of go back to our diagram here. So it searched for memory and agents. It searched our summaries. It found the summary related to memory and agents. It uses that doc ID to reference between the vector store and the doc store. It fishes out the right full doc, returns us the full document, in this case, the full webpage. That's really it. Simple idea, nice way to go from basically like nice, simple proposition style or summary style indexing to full document retrieval, which is very useful, especially with long context LLMs. Thank you.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 11:\n",
      "Hey, this is Lance from LangChain. This is the fourth short video in our RAG from Scratch series that's going to be focused on generation. Now, in the past few videos, we walked through the general flow for kind of basic RAG, starting with indexing, followed by retrieval, then generation of an answer based upon the documents that we retrieved that are relevant to our question. This is kind of the very basic flow. Now, an important consideration in generation is really what's happening is we're taking the documents we retrieve and we're stuffing them into the LLM context window. So if we kind of walk back through the process, we take documents, we split them for convenience or embedding, we then embed each split, and we store that in a vector store as this kind of easily searchable numerical representation or vector. And we take a question, embed it to produce a similar kind of numerical representation. We can then search, for example, using something like K and N in this kind of high dimensional space for documents that are similar to our question based on their proximity or location in this space. In this case, you can see 3D as a toy, kind of toy example. Now we've recovered relevant splits to our question. We pack those into the context window and we produce our answer. Now this introduces the notion of a prompt. So the prompt is kind of a placeholder that has, for example, you know, in our case, keys. So those keys can be like context and question. So they're basically are like buckets that we're going to take those retrieved documents and slot them in. We're going to take our question and also slot it in. And if you kind of walk through this flow, you can kind of see that we can build like a dictionary from our retrieved documents and from our question. And then we can basically populate our prompt template with the values from the dict and then becomes a prompt value, which can be passed to an LLM like a chat model resulting in chat messages, which we then parse into a string and get our answer. So that's like the basic workflow that we're going to see. And let's just walk through that in code very quickly to kind of give you like a hands on intuition. So we had our notebook we walked through previously, install a few packages. I'm setting a few Langsmith environment variables. We'll see it's nice for kind of observing and debugging our traces. Previously we did this quick start. We're going to skip that over. And what I will do is I'm going to build our retriever. So again, I'm going to take documents and load them. And then I'm going to split them here. We've kind of done this previously. So I'll go through this kind of quickly. And then we're going to embed them and store them in our index. So now we have this retriever object here. Now I'm going to jump down here. Now here's where it's kind of fun. This is the generation bit. And you can see here I'm defining something new. This is a prompt template. And my prompt template is something really simple. It's just going to say answer the following question based on this context. It's going to have this context variable and a question. So now I'm building my prompt. So great. Now I have this prompt. I'll define an LLM. I'll choose 3, 5. Now this introduces the notion of a chain. So in LangChain, we have an expression language called L-C-E-L, LangChain Expression Language, which lets you really easily compose things like prompts, LLMs, parsers, retrievers, and other things. But the very simple kind of example here is just let's just take our prompt, which you defined right here, and connect it to an LLM, which you defined right here into this chain. So there's our chain. And all we're doing is we're invoking that chain. So every LangChain Expression Language chain has a few common methods, like invoke, batch, stream. In this case, we just invoke it with a dict. So context and question. That maps to the expected keys here in our template. And so if we run invoke, what we see is it's just going to execute that chain and we get our answer. Now if we zoom over to LangSmith, we should see that it's been populated. So yeah, we see a very simple runnable sequence. Here was our document. And here's our output. And here is our prompt. Answer the following question based on the context. Here's the document we passed in. Here is the question. And then we get our answer. So that's pretty nice. Now there's a lot of other options for rag prompts. I'll pull one in from our prompt tub. This one's kind of a popular prompt. So it just has a little bit more detail. But the main intuition is the same. You're passing in documents. You're asking the alum to reason about the documents, giving a question, produce an answer. And now here I'm going to find a rag chain, which will automatically do the retrieval for us. And all I have to do is specify, here's my retriever, which we defined before. Here is our question, which we invoke with. The question gets passed through to the key question in our dict. And it automatically will trigger the retriever, which will return documents, which get passed into our context. So it's exactly what we did up here, except before we did this manually. And now this is all kind of automated for us. We pass that dict, which is auto-populated, into our prompt, LM, output parser. And now let's invoke it. And that should all just run. And great. We get an answer. And we can look at the trace. And we can see everything that happened. So we can see our retriever was run. These documents were retrieved. They get passed into our LM. And we get our final answer. So this is kind of the end of our overview, where we talked about, I'll go back to the slides here quickly. We talked about indexing, retrieval, and now generation. And follow-up short videos will kind of dig into some of the more complex or detailed themes that address some limitations that can arise in this very simple pipeline. Thanks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 12:\n",
      "Over the next few videos, we're going to be talking about query translation, and in this first video, we're going to cover the topic of multi-query. So query translation sits kind of at the first stage of an advanced RAG pipeline, and the goal of query translation is really to take an input user question and to translate it in some way in order to improve retrieval. So the problem statement is pretty intuitive. User queries can be ambiguous, and if the query is poorly written, because we're typically doing some kind of semantic similarity search between the query and our documents, if the query is poorly written or ill-posed, we won't retrieve the proper documents from our index. So there's a few approaches to attack this problem, and you can kind of group them in a few different ways. So here's one way I like to think about it. A few approaches involve query rewriting. So taking a query and reframing it, like writing it from a different perspective, and that's what we're going to talk about a little bit here in depth using approaches like multi-query or RAG Fusion, which we'll talk about in the next video. You can also do things like take a question and break it down to make it less abstract, look into sub-questions, and there's a bunch of interesting papers focused on that, like Least to Most from Google. You can also take the opposite approach of take a question and make it more abstract, and there's actually an approach we're going to talk about later in a future video called step-back prompting that focuses on a higher-level question from the input. So the intuition, though, for this multi-query approach is we're taking a question and we're going to break it down into a few differently worded questions from different perspectives. And the intuition here is simply that it is possible that the way a question is initially worded, once embedded, it is not well-aligned or in close proximity in this high-dimensional embedding space to a document that we want to retrieve that's actually related. So the thinking is that by kind of rewriting it in a few different ways, you actually increase the likelihood of actually retrieving the document that you really want to. Because of nuances in the way that documents and questions are embedded, this kind of more shotgun approach of taking a question, fanning it out into a few different perspectives may improve and increase the reliability of retrieval. That's like the intuition, really. And of course, we can combine this with retrieval, so we can take our kind of fanned-out questions, do retrieval on each one, and combine them in some way and perform RAG. So that's kind of the overview. And now let's go over to our code. So this is a notebook, and we're going to share all this. We're just installing a few packages. We're setting our Langsmith API keys, which we'll see why that's quite useful here shortly. There's our diagram. Now, first, I'm going to index this blog post on agents. I'm going to split it. Well, I'm going to load it, I'm going to split it, and then I'm going to index it in Chroma locally. So this is a vector store. We've done this previously, so now I have my index defined. So here's where I'm defining my prompt for multi-query, which is you're an assistant, your task is to basically reframe this question into a few different sub-questions. So there's our prompt right here. We'll pass that to an LLM. We'll parse it into a string and then split the string by new lines, and so we'll get a list of questions out of this chain. That's really all we're doing here. Now all we're doing is here's a sample input question. There's our generate queries chain, which we defined. We're going to take that list and then simply apply each question to a retriever. So we'll do retrieval per question. And this little function here is just going to take the unique union of documents across all those retrievals. So let's run this and see what happens. So we're going to run this and we're going to get some set of questions or documents back. So let's go to Langsdorf now. We can actually see what happened under the hood. So here's the key point. We ran our initial chain to generate a set of reframed questions from our input. And here is that prompt, and here is that set of questions that we generated. Now what happened is for every one of those questions, we did an independent retrieval. That's what we're showing here. So that's kind of the first step, which is great. Now I can go back to the notebook and we can show this working end to end. So now we're going to take that retrieval chain. We'll pass it into context of our final rag prompt. We'll also pass through the question. We'll pass that to our rag prompt here, pass it to an LLM, and then parse the output. Now let's kind of see how that works. So again, that's okay. There it is. So let's actually go into Langsdorf and see what happened under the hood. So this was our final chain. This is great. We took our input question. We broke it down to these like five rephrase questions. For every one of those, we did a retrieval. That's all great. We then took the unique union of documents, and you can see in our final LLM prompt, answer the following question based on the context. This is the final set of unique documents that we retrieved from all of our sub questions. Here's our initial question. There's our answer. So that kind of shows you I can set this up really easily. I can use Langsdorf to kind of investigate what's going on, and in particular, use Langsdorf to investigate those intermediate questions that you generate in that like kind of question generation phase, and in future talks, we're going to go through some of these other methods that we kind of introduced at the start of this one. Thank you.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 13:\n",
      "This is the second video of our deep dive on query translation in our Rack and Scratch series focused on a method called Rack Fusion. So as we kind of showed before, query translation you can think of as the first stage in an advanced Rack pipeline where we're taking an input user question and we're translating in some way in order to prove retrieval. Now we showed this general mapping of approaches previously. So again you have kind of like rewriting so you can take a question and kind of break it down into differently worded or different perspectives of the same question. So that's kind of rewriting. There's sub-questions where you take a question and break it down into smaller problems, solve each one independently. And then there's step back where you take a question and kind of go more abstract where you kind of ask a higher level question as a precondition to answer the user question. So those are the approaches and we're going to dig into one of the particular approaches for rewriting called Rack Fusion. Now this is really similar to what we just saw with multi-query. The difference being we actually apply a kind of a clever ranking step of our retrieved documents which we call reciprocal rank fusion. That's really the only difference. The input stage of taking a question, breaking it out into a few kind of differently worded questions, retrieval on each one is all the same. And we're going to see that in the code here shortly. So let's just hop over there and then look at this. So again here is a notebook that we introduced previously. Here's the packages we've installed. We've set a few API keys for a Langsmith which we see why it's quite useful. And you can kind of go down here to a Rack Fusion section. And the first thing you'll note is what our prompt is. So it looks really similar to the prompt we just saw with multi-query and simply your helpful assistant that generates multiple search queries based upon a user input. And here's the question output for queries. So let's define our prompt. And here is our query generation chain. Again, this looks a lot like we just saw. We take our prompt, plumb that into an LLM, and then basically parse by new lines. And that'll basically split out these questions into a list. That's all that's going to happen here. So that's cool. Now here's where the novelty comes in. Each time we do retrieval from one of those questions, we're going to get back a list of documents from our retriever. And so we do it over that. We generate four questions here based on our prompt. We do those four questions, like a list of lists, basically. Now Reciprocal Rank Fusion is really well suited for this exact problem. We want to take this list of lists and build a single consolidated list. And really all that's going on is it's looking at the documents in each list and kind of aggregating them into a final output ranking. And that's really the intuition around what's happening here. So let's go ahead and look at that in some detail. So we can see we run retrieval. That's great. Let's go over to LangSmith and have a look at what's going on here. So we can see that here is our prompt, your helpful assistant that generates multiple search queries based on a single input. And here is our search queries. And then here are our four retrievals. So that's really good. So we know that all is working. And then those retrievals simply went into this rank function and are correspondingly ranked to a final list of six unique ranked documents. That's really all we did. So let's actually put that all together into a full rag chain that's going to run retrieval, return that final list of ranked documents, and pass it to our context, pass through our question, send that to our rag prompt, pass it to an LM, parse it to an output, and let's run all that together and see that working. Cool. So there's our final answer. Now let's have a look in LangSmith. We can see here was our four questions. Here's our retrievals. And then our final rag prompt plumbed through the final list of ranked six questions, which we can see laid out here, and our final answer. So this can be really convenient, particularly if we're operating across, like, maybe different vector stores or we want to do, like, retrieval across a large number of kind of differently worded questions. This reciprocal rank fusion step is really nice. For example, if we wanted to only take the top three documents or something, it can be really nice to build that consolidated ranking across all these independent retrievals and pass that to the LM for the final generation. So that's really the intuition about what's happening here. Thanks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original transcript of the video Nr. 14:\n",
      "Hi, this is Lance from LangChain. This is the 14th part of our RAG from Scratch series. I'm going to be talking about an approach called Colbert. So we've talked about a few different approaches for indexing. And just as kind of a refresher, indexing falls kind of right down here in our flow. We started initially with query translation, taking a question, translating it in some way to optimize retrieval. We talked about routing it to a particular database. We then talked about query construction, so going from natural language to the DSL or domain-specific language for any of the databases that you want to work with. Those are metadata filters for vector stores or Cypher for GraphDB or SQL for relational DB. So that's kind of the flow we talked about today. We talked about some indexing approaches like multi-representation indexing. We gave a small shout-out to Greg Cameron in the series on chunking. We talked about hierarchical indexing. I want to include one advanced kind of embedding approach. So we talked a lot about embeddings are obviously very central to semantic similarity search and retrieval. So one of the interesting points that's been brought up is that embedding models, of course, take a document, you can see here on the top, and embed it, basically compress it to a vector. So it's kind of a compression process. You're representing all the semantics of that document in a single vector. You're doing the same to your question. You're doing a similarity search between the question embedding and the document embedding in order to perform retrieval. You're typically taking the K most similar document embedding is given a question, and that's really how you're doing it. Now, a lot of people have said, well, hey, compressing a full document with all this nuance to a single vector seems a little bit overly restrictive, right? And this is a fair question to ask. There's been some interesting approaches to try to address that. One is this approach method called Colbert. So the intuition is actually pretty straightforward. There's a bunch of good articles I linked down here. This is my little cartoon to explain it, which I think is hopefully kind of helpful. But here's the main idea. Instead of just taking a document and compressing it down to a single vector, basically a single what we might call embedding vector, we take the document, we break it up into tokens. So tokens are just like, you know, units of content. It depends on the token you use. We talked about this earlier. So you basically tokenize it. And you produce basically an embedding or a vector for every token. And there's some kind of positional weighting that occurs when you do this process. So obviously you look at the implementation to understand the details. But the intuition is that you're producing some kind of representation for every token. Okay? And you're doing the same thing for your questions. You're taking your question, you're breaking it into tokens, and you have some representation or vector per token. And then what you're doing is for every token in the question, you're computing the similarity across all the tokens in the document. And you're finding the max. So you're taking the max, you're storing that, and you're doing that process for all the tokens in the question. So, again, token two, you compare it to every token in the document, compute the max. And then the final score is, in this case, the sum of the max similarities between every question token and any document token. So it's an interesting approach. It reports very strong performance. Latency is definitely a question. So kind of production readiness is something you should look into. But it's an approach that's worth mentioning here because it's pretty interesting. And let's walk through the code. So there's actually a really nice library called Rakutui, which makes it very easy to play with Colbert. Just pip install it here. I've already done that. And we can use one of their pre-trained models to mediate this process. So I'm basically following their documentation. This is kind of what they recommended. So I'm running this now. Hopefully this runs somewhat quickly. I'm not sure. I previously have loaded this model, so hopefully it won't take too long. And, yeah, you can see it's pretty quick. I'm on a Mac M2 with 32 gigs, so just as like a context in terms of my system. This is from their documentation. We're just grabbing a Wikipedia page. This is getting a full document on Miyazaki. So that's cool. We're going to grab that. Now this is just from their docs. This is basically how we create an index. So we provide the, you know, some index name, the collection, the .max document length. And, yeah, you should look at their documentation for these flags. These are just the defaults. So I'm going to create my index. So I get some logging here. So it's working under the hood. And, by the way, I actually have their documentation open, so you can kind of follow along. So let's see. Yeah, right about here. So you can kind of follow this indexing process to create an index. So you need to load a trained model. This can be either your own pre-trained model or one of ours from the hub. And this is kind of the process we're doing right now. Create index is just a few lines of code, and this is exactly what we're doing. So this is the, you know, my documents. And this is the indexing step that we just kind of walked through. And it looks like it's done. So you get a bunch of logging here. That's fine. Now let's actually see if this works. So we're going to run drag search. What in Motion Studio did Miyazaki found? It's at our K parameter. And we get some results. Okay, so it's running. And, cool, we get some documents out. So, you know, it seems to work. Now what's nice is you can run this within LangChain as a LangChain retriever. So that basically wraps this as a LangChain retriever. And then you can use it freely as a retriever within LangChain. It works with all the other different LMs and all the other components, like re-rankers and so forth, that we talked through. So you can use this directly as a retriever. Let's try this out. And, boom, nice and fast. And we get our documents again. This is a super simple test example. You should run this maybe on more complex cases. But it's a pretty easy spin-up. It's a really interesting alternative indexing approach. Using, again, like we talked through, a very different algorithm for computing doc similarity that may work better. I think an interesting regime to consider this would be longer documents. So if you want, like, longer, yeah, if you basically want kind of long context embedding, I think you should look into, for example, the max token limits for this approach because it partitions a document into each token. I would be curious to dig into kind of what the overall context limits are for this approach of Colbert. But it's really interesting to consider. And it reports very strong performance. So, again, I encourage you to play with it. And this is just kind of an intro to how to get set up and to start experimenting with it really quickly. Thanks.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# video order doesn't match as batch_size parameter is not supported (see llm above)\n",
    "output_content(map_reduce_outputs['input_documents'], 'Original transcript of the video')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
